***

# ОТЧЕТ

## Лабораторная работа №2: Коллективные операции MPI (скалярное произведение и Aᵀ·x)

### Сведения о студенте

* **Дата:** 2025-12-28
* **Семестр:** 1
* **Группа:** ПИН-м-о-25-1 (1)
* **Дисциплина:** Параллельные вычисления
* **Студент:** Мизин Глеб Егорович

***

## 1. Цель работы

Освоить коллективные операции MPI в `mpi4py` и реализовать две параллельные задачи:

1. вычисление скалярного произведения вектора на себя `a·a`;
2. вычисление произведения транспонированной матрицы на вектор `b = Aᵀ·x`.

---

## 2. Задание и идея алгоритмов

### 2.1. Задача 1: скалярное произведение a·a

* На root создаётся вектор `a`.
* Вектор делится на части между процессами через `Scatterv`.
* Каждый процесс считает локальную сумму `local = dot(a_part, a_part)`.
* Общая сумма собирается на root через `Reduce(SUM)`.

Использовано: `bcast` (для counts/displs), `Scatterv`, `Reduce`.

---

### 2.2. Задача 2: умножение транспонированной матрицы на вектор b = Aᵀ·x

* Матрица `A` размера `M×N`, вектор `x` длины `M`.
* Матрица делится по строкам на блоки `A_part` через `Scatterv`.
* Вектор `x` делится согласованно по строкам на `x_part` через `Scatterv`.
* Каждый процесс вычисляет вклад в результат:

  * `b_temp = A_part.T @ x_part` (вектор длины `N`)
* Итоговый результат получается суммированием вкладов всех процессов:

  * `b = sum(b_temp)` по процессам через `Reduce(SUM)`.

Почему `Reduce`, а не `Gatherv`: нужно не “склеить куски”, а **сложить** векторы `b_temp` поэлементно.

Использовано: `bcast` (counts/displs), `Scatterv`, `Reduce`, `Barrier`, `Wtime`.

---

## 3. Окружение

* ОС: Windows
* MPI: MS-MPI (`mpiexec`)
* Python: 3.10.7
* NumPy: 2.2.6
* mpi4py: 4.1.1

---

## 4. Реализация и запуск

### 4.1. Файлы проекта

* `lr2_dot.py` — скалярное произведение `a·a`
* `lr2_Atx.py` — вычисление `b = Aᵀ·x`

### 4.2. Команды запуска

```bash
mpiexec -n 4 python lr2_dot.py
mpiexec -n 4 python lr2_Atx.py
```

---

## 5. Результаты

### 5.1. Скалярное произведение

Запуск:

```text
dot(a,a) = 9000045000050000.0
```

### 5.2. Умножение транспонированной матрицы на вектор

Параметры задачи в коде: `M = 10000`, `N = 200`.

Запуск на 4 процессах:

```text
compute time ~ 0.000556 s
max abs diff vs seq = 2.000888343900442e-11
```

Вывод: параллельный результат совпадает с последовательным с высокой точностью (ошибка порядка 1e-11).

---

## 6. Заключение

В работе реализованы две MPI-программы с использованием коллективных операций:

* `Scatterv` для распределения данных между процессами,
* `Reduce(SUM)` для суммирования частичных результатов.

Для задачи `b = Aᵀ·x` корректность подтверждена сравнением с последовательным вычислением.

---