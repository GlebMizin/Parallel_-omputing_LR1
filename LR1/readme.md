---

# ОТЧЕТ

## По лабораторной работе №1: Параллельное умножение матрицы на вектор (MPI, Scatterv/Gatherv) на Python

### Сведения о студенте

**Дата:** 2025-12-28
**Семестр:** 1
**Группа:** ПИН-м-о-25-1 (1)
**Дисциплина:** Параллельные вычисления
**Студент:** Мизин Глеб Егорович

---

## 1. Цель работы

Освоить базовые принципы программирования в модели передачи сообщений (Message Passing) с использованием MPI (библиотека `mpi4py` для Python) и реализовать параллельный алгоритм умножения матрицы на вектор с последующим измерением времени работы и оценкой ускорения.

---

## 2. Теоретическая часть

### 2.1. Основные понятия и алгоритмы

Дана матрица `A` размера `M×N` и вектор `x` размера `N`. Требуется вычислить вектор результата `b` размера `M`:

`b = A · x`

Параллельный алгоритм использует разбиение матрицы **по строкам** (полосами):

1. Процесс `0` (root) читает размеры `M, N`, матрицу `A` и вектор `x`.
2. Размеры `M, N` рассылаются всем процессам.
3. Вектор `x` рассылается всем процессам (Broadcast).
4. Матрица `A` распределяется между процессами блоками строк:

   * каждый процесс получает `A_part` размера `local_M×N`.
5. Каждый процесс вычисляет локальную часть результата:

   * `b_part = A_part · x`
6. Локальные части `b_part` собираются на root в итоговый `b`.

Если `M` не делится на число процессов `p`, используются массивы:

* `rcounts[i]` — сколько строк получает процесс `i`
* `displs[i]` — смещение (первая строка блока) процесса `i`

Для распределения матрицы числа отправляемых **элементов** считаются как `rcounts[i] * N`, а смещения — как `displs[i] * N`.

---

### 2.2. Используемые функции MPI

В программе использовались следующие операции MPI (через `mpi4py`):

* `MPI.COMM_WORLD` — основной коммуникатор.
* `comm.Get_rank()` — номер процесса (rank).
* `comm.Get_size()` — количество процессов.
* `comm.bcast(obj, root=0)` — рассылка скалярных значений (размеры `M`, `N`) как Python-объектов.
* `comm.Bcast(buf, root=0)` — рассылка массивов (вектор `x`) всем процессам.
* `comm.Scatterv(...)` — распределение блоков матрицы `A` разного размера.
* `comm.Gatherv(...)` — сбор частей результата `b_part` разного размера.
* `comm.Barrier()` — синхронизация процессов.
* `MPI.Wtime()` — измерение времени.

---

## 3. Практическая реализация

### 3.1. Структура программы

В проекте использовались файлы:

* `gen_data.py` — генерация входных данных (`in.dat`, `AData.dat`, `xData.dat`)
* `seq_matvec.py` — последовательная проверочная версия
* `mpi_scatterv_gatherv.py` — параллельная версия (Scatterv/Gatherv)
* `test_mpi.py` — проверка, что MPI окружение работает

Формат входных файлов:

* `in.dat`: первая строка `N`, вторая строка `M`
* `AData.dat`: элементы матрицы `A` (по строкам)
* `xData.dat`: элементы вектора `x`

---

### 3.2. Ключевые особенности реализации

1. **Корректная работа при `M % p != 0`**
   Используются `Scatterv/Gatherv` и массивы `rcounts/displs`, чтобы процессы могли получать разное число строк.

2. **Проблема broadcast numpy-скаляров и решение**
   При попытке сделать `Bcast` для numpy-скаляра возникала ошибка вида `BufferError: scalar buffer is readonly`.
   Решение: размеры `M, N` рассылались через `comm.bcast()` (Python-объекты), а массивы (вектор `x`) — через `Bcast`.

3. **Измерение времени**
   Время измерялось через `MPI.Wtime()` вокруг участка вычисления `b_part = A_part @ x`, с `Barrier()` до и после.

---

### 3.3. Инструкция по запуску

Команды (PowerShell, активирован venv):

```bash
# Проверка версий
python -V
python -c "import numpy, mpi4py; print(numpy.__version__); print(mpi4py.__version__)"

# Запуск параллельной версии
mpiexec -n 4 python mpi_scatterv_gatherv.py
```

Пример успешного запуска:

```text
compute time ~ 0.000075 s, wrote Results_parallel.dat
```

---

## 4. Экспериментальная часть

### 4.1. Тестовые данные

Тестовые данные генерировались скриптом `gen_data.py` (файлы `in.dat`, `AData.dat`, `xData.dat`).
В рамках текущего прогона замеры выполнены для одного набора данных (задача A).

---

### 4.2. Методика измерений

* Измерялось время **только вычислительной части** (локальное умножение блока матрицы на вектор).
* Перед стартом и после окончания вычисления использовалась синхронизация `Barrier()`.
* Время выводилось в консоль строкой вида `compute time ~ ... s`.

Окружение эксперимента:

* Python: 3.10.7
* NumPy: 2.2.6
* mpi4py: 4.1.1
* MPI: MS-MPI (Windows)

---

### 4.3. Результаты измерений

#### Таблица 1. Время выполнения (секунды)

| Количество процессов | Размер задачи A | Размер задачи B | Размер задачи C |
| -------------------- | --------------- | --------------- | --------------- |
| 1                    | 0.000019        | —               | —               |
| 2                    | 0.000016        | —               | —               |
| 4                    | 0.000059        | —               | —               |
| 8                    | 0.000165        | —               | —               |

#### Таблица 2. Ускорение (Speedup)

Ускорение считалось как `S(p) = T(1) / T(p)`.

| Количество процессов | Размер задачи A | Размер задачи B | Размер задачи C |
| -------------------- | --------------- | --------------- | --------------- |
| 1                    | 1.00            | 1.00            | 1.00            |
| 2                    | 1.19            | —               | —               |
| 4                    | 0.32            | —               | —               |
| 8                    | 0.12            | —               | —               |

---

## 5. Визуализация результатов

### 5.1. График времени выполнения

График рекомендуется построить по данным из Таблицы 1 и сохранить в `images/execution_time.png`.

### 5.2. График ускорения

График рекомендуется построить по данным из Таблицы 2 и сохранить в `images/speedup.png`.

### 5.3. График эффективности

Эффективность рекомендуется считать как `E(p) = S(p) / p` и сохранить график в `images/efficiency.png`.

---

## 6. Анализ результатов

### 6.1. Анализ производительности

Для очень маленьких размеров задачи время вычислений получается микроскопическим, и на первый план выходят:

* накладные расходы MPI (Scatterv/Gatherv, синхронизации),
* накладные расходы Python,
* шум измерений (очень маленькие значения времени).

Из-за этого при `p=4` и `p=8` ускорение меньше 1 (наблюдается замедление).

### 6.2. Сравнение с теоретическими оценками

Теоретически при увеличении числа процессов ожидается ускорение, однако закон Амдала и стоимость обменов/синхронизаций ограничивают рост производительности. При малых размерах задачи коммуникации доминируют над вычислениями, поэтому ускорение может ухудшаться.

### 6.3. Выявление узких мест

Основные ограничивающие факторы:

* затраты на коллективные операции `Scatterv/Gatherv`,
* барьеры синхронизации,
* слишком маленький объём вычислений (задача не “нагружает” процессор).

---

## 7. Ответы на контрольные вопросы

### Вопрос 1: Что такое rank и size в MPI?

`rank` — номер процесса в коммуникаторе. `size` — общее число процессов.

### Вопрос 2: Чем Send/Recv отличается от коллективных операций?

Send/Recv — обмен “процесс↔процесс”. Коллективные операции (Bcast/Scatter/Gather) выполняются группой процессов и обычно оптимизированы.

### Вопрос 3: Зачем рассылать вектор x всем процессам?

Вектор `x` одинаковый для всех и нужен каждому процессу для вычисления `b_part = A_part · x`.

### Вопрос 4: Почему используется Scatterv/Gatherv?

Потому что блоки строк могут быть разного размера, если `M` не делится на число процессов.

### Вопрос 5: Для чего нужны rcounts и displs?

`rcounts` задаёт объём данных для каждого процесса, `displs` — смещения блоков в исходном массиве.

### Вопрос 6: Зачем Barrier в измерениях?

Чтобы все процессы начали/закончили измеряемый участок одновременно, иначе время будет некорректным.

### Вопрос 7: Почему ускорение может быть меньше 1?

Из-за накладных расходов на коммуникации и синхронизацию, особенно при маленьком объёме вычислений.

### Вопрос 8: Как проверить корректность результата?

Сравнить параллельный результат с последовательным (например, по максимальной разнице элементов).

### Вопрос 9: Какие типы MPI используются для массивов NumPy?

Для `float64` обычно используется `MPI.DOUBLE`, для целых значений счётчиков — `MPI.INT` (или совместимые типы).

### Вопрос 10: Что улучшить, чтобы получить нормальное ускорение?

Использовать большие размеры матриц, уменьшить долю коммуникаций, оптимизировать распределение данных и измерять время на нескольких прогонах с усреднением.

---

## 8. Заключение

### 8.1. Выводы

Реализовано параллельное умножение матрицы на вектор на Python с использованием MPI (Scatterv/Gatherv), выполнены замеры времени на 1/2/4/8 процессах.

### 8.2. Проблемы и решения

Проблема: ошибка при broadcast numpy-скаляров (`BufferError: scalar buffer is readonly`).
Решение: рассылка размеров `M, N` через `comm.bcast()`, а массивов — через `Bcast`.

### 8.3. Перспективы улучшения

* провести замеры на больших размерах задач (A/B/C),
* построить графики времени, ускорения и эффективности,
* сравнить с вариантом на Send/Recv (если требуется по заданию).

---

## 9. Приложения

### 9.1. Исходный код

Файлы проекта в репозитории:

* `gen_data.py`
* `seq_matvec.py`
* `mpi_scatterv_gatherv.py`
* `test_mpi.py`

### 9.2. Используемые библиотеки и версии

* Python 3.10.7
* mpi4py 4.1.1
* NumPy 2.2.6
* MS-MPI (Windows)

### 9.3. Рекомендуемая литература

1. Документация mpi4py — примеры коллективных операций и работа с NumPy.
2. Стандарт MPI — описание `Bcast`, `Scatterv`, `Gatherv`.
3. Материалы лекций по курсу “Параллельные вычисления”.

---

*Отчет подготовлен в рамках курса "Параллельные вычисления"*

---
