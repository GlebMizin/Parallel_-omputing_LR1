***

# ОТЧЕТ

## Лабораторная работа №3: Метод сопряжённых градиентов (CG) для МНК в MPI (mpi4py)

### Сведения о студенте

* **Дата:** 2025-12-28
* **Семестр:** 1
* **Группа:** ПИН-м-о-25-1 (1)
* **Дисциплина:** Параллельные вычисления
* **Студент:** Мизин Глеб Егорович

***

## 1. Цель работы

Реализовать параллельный алгоритм метода сопряжённых градиентов (CG) для решения задачи наименьших квадратов `min ||A x - b||` с использованием MPI (mpi4py). Проверить корректность решения сравнением с `numpy.linalg.lstsq`, снять время выполнения для разного числа процессов.

---

## 2. Постановка задачи

Дана матрица `A` размера `M×N` и вектор `b` длины `M`. Требуется найти вектор `x` длины `N`, минимизирующий норму невязки:

`min ||A x - b||`

Решение вычисляется методом CG, применённым к нормальным уравнениям:

`(A^T A) x = A^T b`

---

## 3. Идея параллельного алгоритма

1. Матрица `A` и вектор `b` распределяются **по строкам** между процессами (каждый процесс хранит `A_part` и `b_part`).
2. Вектор `x` хранится распределённо по координатам (часть `x_part`), а для вычислений периодически собирается полный вектор через коллективную операцию.
3. Для вычисления `A^T b` и `A^T (A p)` используются коллективные операции суммирования по процессам.
4. Итоговый `x` собирается на root, где сравнивается с решением `numpy.linalg.lstsq`.

Использованные коллективные операции:

* `Scatterv` — раздать части `A` и `b`;
* `Allgatherv` — собрать полный вектор направления `p` (нужен для умножения `A_part @ p`);
* `Allreduce(SUM)` — суммировать векторы/скаляры, например для `A^T b`, `A^T(Ap)`, а также для скалярных произведений;
* `Gatherv` — собрать итоговый `x` на root.

---

## 4. Окружение

* ОС: Windows
* MPI: MS-MPI (`mpiexec`)
* Python: 3.10.7
* NumPy: 2.2.6
* mpi4py: 4.1.1

---

## 5. Запуск

Команды (PowerShell, активирован venv):

```bash
mpiexec -n 1 python lr3_cg.py
mpiexec -n 2 python lr3_cg.py
mpiexec -n 4 python lr3_cg.py
mpiexec -n 8 python lr3_cg.py
```

Параметры задачи (из запуска):

* `N = 200` (число неизвестных)
* `M = 300` (число уравнений)

---

## 6. Результаты

### 6.1. Корректность

Сравнение с `numpy.linalg.lstsq`:

* `max|x_cg - x_lstsq|` порядка `4.2e-11` (очень маленькая ошибка)
* нормы невязок совпадают:

  * `||Ax_cg - b|| = 1.076e-01`
  * `||Ax_lstsq - b|| = 1.076e-01`

Вывод: решение CG совпадает с эталонным решением NumPy с высокой точностью.

---

### 6.2. Время и ускорение

#### Таблица 1 — Время (core_time)

Время — это время основного цикла CG (без генерации и чтения файлов).

| Процессов | Итераций | core_time (с) |
| --------: | -------: | ------------: |
|         1 |       98 |      0.003139 |
|         2 |       98 |      0.003297 |
|         4 |       98 |      0.003479 |
|         8 |       98 |      0.006662 |

#### Таблица 2 — Ускорение (Speedup)

`S(p) = T(1) / T(p)`

| Процессов | Speedup |
| --------: | ------: |
|         1 |    1.00 |
|         2 |    0.95 |
|         4 |    0.90 |
|         8 |    0.47 |

---

## 7. Анализ

Для данных размеров (`N=200`, `M=300`) задача очень маленькая, поэтому накладные расходы MPI (Allreduce/Allgatherv, синхронизации, обмен данными) сравнимы или больше, чем время вычислений. Из-за этого ускорение меньше 1 (наблюдается замедление при увеличении числа процессов). Для получения реального ускорения нужны большие размеры матрицы.

---

## 8. Заключение

Реализован параллельный метод CG для решения задачи наименьших квадратов в MPI (mpi4py). Корректность подтверждена сравнением с `numpy.linalg.lstsq`. Проведены измерения времени для 1/2/4/8 процессов. На малой задаче ускорение не достигается из-за доминирования коммуникационных расходов.

---